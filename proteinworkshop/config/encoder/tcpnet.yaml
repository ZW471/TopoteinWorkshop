_target_: topotein.models.graph_encoders.tcpnet.TCPNetModel
# overrides for feature config #
features:
  # note: will manually be injected into `/features` via `validate_gcpnet_config()` of `proteinworkshop/configs/config.py`
  vector_node_features: ["orientation"]
  vector_edge_features: ["edge_vectors"]
  vector_sse_features: ["sse_vectors", "consecutive_diff", "pr_com_diff"]
  vector_pr_features: ["farest_nodes", "nearest_nodes"]
  neighborhoods:
    - N0_2 = B0_2
    - N2_0 = B0_2.T
    - N0_3 = B0_3
    - N2_3 = B2_3
    - N3_2 = B2_3.T
    - N1_2 = B1_2
#    - N2_1_outer = B0_2.T @ B0_1 - B1_2.T
    - N1_2_outer = (B0_2.T @ B1_0.T - B1_2.T).T
  directed_edges: True
# global config #
num_layers: 6 # Number of layers in the model
emb_dim: 128
node_s_emb_dim: ${.emb_dim} # Dimension of the node state embeddings
node_v_emb_dim: ${divide:${.node_s_emb_dim},4} # Dimension of the node vector embeddings
edge_s_emb_dim: ${divide:${.emb_dim},4} # Dimension of the edge state embeddings
edge_v_emb_dim: ${divide:${.edge_s_emb_dim},4} # Dimension of the edge vector embeddings
sse_s_emb_dim: ${divide:${.emb_dim},2}
sse_v_emb_dim: ${divide:${.sse_s_emb_dim},4}
pr_s_emb_dim: ${.emb_dim}
pr_v_emb_dim: ${divide:${.pr_s_emb_dim},4}
r_max: 10.0 # Maximum distance for radial basis functions
num_rbf: 8 # Number of radial basis functions
activation: silu # Activation function to use in each GCP layer
pool: sum # Global pooling method to be used
# module config #
module_cfg:
  sse_update: true
  pr_update: true
  use_tnn_pooling: true
  norm_pos_diff: true
  scalar_gate: 0
  vector_gate: true
  scalar_nonlinearity: ${..activation}
  vector_nonlinearity: ${..activation}
  nonlinearities:
    - ${..scalar_nonlinearity}
    - ${..vector_nonlinearity}
  r_max: ${..r_max}
  num_rbf: ${..num_rbf}
  bottleneck: 4
  vector_linear: true
  vector_identity: true
  default_bottleneck: 4
  predict_node_positions: false  # note: if `false`, then the input node positions will not be updated
  predict_node_rep: true  # note: if `false`, then a final projection of the node features will not be performed
  node_positions_weight: 1.0
  update_positions_with_vector_sum: false
  enable_e3_equivariance: false
  pool: ${..pool}
# model config #
model_cfg:
  h_input_dim: ${resolve_feature_config_dim:${features},scalar_node_features,${task},true}
  chi_input_dim: ${resolve_feature_config_dim:${features},vector_node_features,${task},false}
  e_input_dim: ${plus:${resolve_feature_config_dim:${features},scalar_edge_features,${task},true},${..num_rbf}}
  xi_input_dim: ${resolve_feature_config_dim:${features},vector_edge_features,${task},false}
  c_input_dim: ${resolve_feature_config_dim:${features},scalar_sse_features,${task},true}
  rho_input_dim: ${resolve_feature_config_dim:${features},vector_sse_features,${task},false}
  p_input_dim: ${resolve_feature_config_dim:${features},scalar_pr_features,${task},true}
  pi_input_dim: ${resolve_feature_config_dim:${features},vector_pr_features,${task},false}
  # note: each `hidden_dim` must be evenly divisible by `bottleneck`
  h_hidden_dim: ${..node_s_emb_dim}
  chi_hidden_dim: ${..node_v_emb_dim}
  e_hidden_dim: ${..edge_s_emb_dim}
  xi_hidden_dim: ${..edge_v_emb_dim}
  c_hidden_dim: ${..sse_s_emb_dim}
  rho_hidden_dim: ${..sse_v_emb_dim}
  p_hidden_dim: ${..pr_s_emb_dim}
  pi_hidden_dim: ${..pr_v_emb_dim}
  num_layers: ${..num_layers}
  dropout: 0.0
# layer config #
layer_cfg:
  pre_norm: false
  use_gcp_norm: true
  use_gcp_dropout: true
  use_scalar_message_attention: true
  num_feedforward_layers: 2
  dropout: 0.0
  nonlinearity_slope: 1e-2
  # message-passing config #
  mp_cfg:
    edge_encoder: false
    edge_gate: false
    num_message_layers: 4
    message_residual: 0
    message_ff_multiplier: 1
    self_message: true
